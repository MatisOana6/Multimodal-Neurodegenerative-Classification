{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ca566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python-headless\n",
    "%pip install opencv-python\n",
    "%pip install scikit-image\n",
    "%pip install scikit-learn\n",
    "%pip install tqdm\n",
    "%pip install sympy==1.13.3\n",
    "%pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSpectrogramDataset(Dataset):\n",
    "    def __init__(self, wav_dir, spec_dir, classes, segment_len=160000):\n",
    "        self.wav_dir = wav_dir\n",
    "        self.spec_dir = spec_dir\n",
    "        self.classes = classes\n",
    "        self.segment_len = segment_len\n",
    "        self.data = []\n",
    "\n",
    "        for cls_index, cls_name in enumerate(classes):\n",
    "            wav_cls_path = os.path.join(wav_dir, cls_name)\n",
    "            spec_cls_path = os.path.join(spec_dir, cls_name)\n",
    "\n",
    "            for filename in os.listdir(wav_cls_path):\n",
    "                if filename.endswith(\".wav\"):\n",
    "                    base = os.path.splitext(filename)[0]\n",
    "                    wav_path = os.path.join(wav_cls_path, filename)\n",
    "                    spec_path = os.path.join(spec_cls_path, base + \".npy\")\n",
    "\n",
    "                    if os.path.exists(spec_path):\n",
    "                        self.data.append((wav_path, spec_path, cls_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path, spec_path, label = self.data[idx]\n",
    "\n",
    "        y, sr = librosa.load(wav_path, sr=16000)\n",
    "        y = y / np.max(np.abs(y)) \n",
    "\n",
    "        if len(y) < self.segment_len:\n",
    "            y = np.pad(y, (0, self.segment_len - len(y)))\n",
    "        else:\n",
    "            y = y[:self.segment_len]\n",
    "\n",
    "        mel = np.load(spec_path)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "            torch.tensor(mel, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "wav_root = \"D:/Licenta/Datasets/Audio/data/Split_Wav/test\"\n",
    "npy_root = \"D:/Licenta/Datasets/Audio/data/MelSpectrograms/test\"\n",
    "\n",
    "missing = []\n",
    "\n",
    "for cls in os.listdir(wav_root):\n",
    "    wav_cls_path = os.path.join(wav_root, cls)\n",
    "    npy_cls_path = os.path.join(npy_root, cls)\n",
    "\n",
    "    for file in os.listdir(wav_cls_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            base = os.path.splitext(file)[0]\n",
    "            npy_file = os.path.join(npy_cls_path, base + \".npy\")\n",
    "            if not os.path.exists(npy_file):\n",
    "                missing.append((cls, base + \".npy\"))\n",
    "\n",
    "if missing:\n",
    "    print(\"Lipsesc fișiere .npy pentru următoarele segmente:\")\n",
    "    for cls, fname in missing:\n",
    "        print(f\" - {cls}/{fname}\")\n",
    "else:\n",
    "    print(\"Toate fișierele .wav au .npy corespunzător.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Alzheimer\", \"Parkinson\", \"Healthy\"]\n",
    "\n",
    "train_dataset = AudioSpectrogramDataset(\n",
    "    wav_dir=\"D:/Licenta/Datasets/Audio/data/Split_Wav/train\",\n",
    "    spec_dir=\"D:/Licenta/Datasets/Audio/data/MelSpectrograms/train\",\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "test_dataset = AudioSpectrogramDataset(\n",
    "    wav_dir=\"D:/Licenta/Datasets/Audio/data/Split_Wav/test\",\n",
    "    spec_dir=\"D:/Licenta/Datasets/Audio/data/MelSpectrograms/test\",\n",
    "    classes=class_names\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples |  Test: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class AudioSpectrogramClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=3, pretrained_resnet=True):\n",
    "        super(AudioSpectrogramClassifier, self).__init__()\n",
    "\n",
    "        \n",
    "        self.audio_branch = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=9, stride=2, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=9, stride=2, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(32),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3) \n",
    "        )\n",
    "\n",
    "        self.spec_branch = models.resnet18(pretrained=pretrained_resnet)\n",
    "        self.spec_branch.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.spec_branch.fc = nn.Identity() \n",
    "\n",
    "        self.fusion_dim = 512 + 64 * 32 \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.fusion_dim, self.fusion_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.fusion_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_input, mel_input):\n",
    "        x1 = self.audio_branch(audio_input.unsqueeze(1))    \n",
    "        x2 = self.spec_branch(mel_input.unsqueeze(1))        \n",
    "        x = torch.cat((x1, x2), dim=1)                       \n",
    "        att = self.attention(x)                              \n",
    "        x = x * att                                           \n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6634dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioSpectrogramClassifier(n_classes=3, pretrained_resnet=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for audio, mel, labels in dataloader:\n",
    "            audio, mel, labels = audio.to(device), mel.to(device), labels.to(device)\n",
    "            outputs = model(audio, mel)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b986c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "best_acc = 0.0\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for audio, mel, labels in train_loader:\n",
    "        audio, mel, labels = audio.to(device), mel.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(audio, mel)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_acc = evaluate(model, train_loader)\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"Model salvat (Test Acc: {best_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, n_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"dual_branch_model.pth\")\n",
    "print(\"Model salvat cu succes în dual_branch_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, wav_path, spec_path, classes):\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    y = y / np.max(np.abs(y))\n",
    "    if len(y) < 160000:\n",
    "        y = np.pad(y, (0, 160000 - len(y)))\n",
    "    else:\n",
    "        y = y[:160000]\n",
    "\n",
    "    mel = np.load(spec_path)\n",
    "    if mel.shape != (128, 128):\n",
    "        raise ValueError(\"Spectrogram should have shape (128, 128)\")\n",
    "\n",
    "    audio_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(0).to(device)  \n",
    "    mel_tensor = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).to(device)  \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(audio_tensor, mel_tensor)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        return classes[pred.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AudioSpectrogramClassifier(n_classes=3).to(device)\n",
    "model.load_state_dict(torch.load(\"dual_branch_model.pth\", map_location=device))\n",
    "\n",
    "class_names = [\"Alzheimer\", \"Parkinson\", \"Healthy\"]\n",
    "\n",
    "wav_path = \"D:/Licenta/Datasets/Audio/data/Split_Wav/test\\Alzheimer/add_reverb_54_adrso232_vol_down_part8.wav\"\n",
    "spec_path = \"D:/Licenta/Datasets/Audio/data/MelSpectrograms/test\\Alzheimer/add_reverb_54_adrso232_vol_up_part8.npy\"\n",
    "\n",
    "predicted_class = predict(model, wav_path, spec_path, class_names)\n",
    "print(\"Predicție:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c930ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_metrics(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audio, mel, labels in dataloader:\n",
    "            audio = audio.to(device)\n",
    "            mel = mel.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(audio, mel)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cr = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "\n",
    "    print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a505cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_with_metrics(model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_wav(model, wav_path, class_names, device=\"cuda\", show_probs=False):\n",
    "    import librosa\n",
    "\n",
    "    n_mels = 128\n",
    "    target_shape = (128, 128)\n",
    "    segment_len = 160000\n",
    "\n",
    "    y, sr = librosa.load(wav_path, sr=16000)\n",
    "    y = y / np.max(np.abs(y))\n",
    "    if len(y) < segment_len:\n",
    "        y = np.pad(y, (0, segment_len - len(y)))\n",
    "    else:\n",
    "        y = y[:segment_len]\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    mel_db = (mel_db - np.mean(mel_db)) / np.std(mel_db)\n",
    "    if mel_db.shape[1] < target_shape[1]:\n",
    "        mel_db = np.pad(mel_db, ((0,0), (0, target_shape[1] - mel_db.shape[1])))\n",
    "    else:\n",
    "        mel_db = mel_db[:, :target_shape[1]]\n",
    "\n",
    "    audio_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    mel_tensor = torch.tensor(mel_db, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(audio_tensor, mel_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "        pred = np.argmax(probs)\n",
    "\n",
    "    print(f\"Predicție: {class_names[pred]}\")\n",
    "    if show_probs:\n",
    "        for i, c in enumerate(class_names):\n",
    "            print(f\"  {c}: {probs[i]:.4f}\")\n",
    "    return class_names[pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120325dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"dual_branch_model.pth\"))\n",
    "predict_from_wav(model, \"D:/Licenta/Datasets/Audio/data/Split_Wav/test\\Parkinson/add_reverb_47_ID18_pd_4_3_3_readtext_pitch_down_part2.wav\", class_names)\n",
    "predict_from_wav(model, \"D:\\Licenta\\Datasets\\Audio\\data\\Augmented_Output\\Alzheimer/add_reverb_20_adrso031_noise.wav\", class_names)\n",
    "predict_from_wav(model, \"d:\\Licenta\\Datasets\\Audio\\data\\Augmented\\Alzheimer/adrso031.wav\", class_names)\n",
    "predict_from_wav(model, \"D:\\Licenta\\Datasets\\Audio\\data\\Augmented_Output\\Healthy/add_reverb_20_adrso286_noise.wav\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def view_npy_spectrogram(npy_path, title=None):\n",
    "    mel = np.load(npy_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mel, x_axis='time', y_axis='mel', cmap='magma')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title or f\"Spectrogram: {npy_path}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_npy_spectrogram(\"D:\\Licenta\\Datasets\\Audio\\data\\MelSpectrograms/test/Alzheimer/add_reverb_19_adrso215_vol_up_part5.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ddbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def view_multiple_spectrograms(npy_paths, titles=None):\n",
    "    plt.figure(figsize=(10, 4 * len(npy_paths)))\n",
    "\n",
    "    for i, path in enumerate(npy_paths):\n",
    "        mel = np.load(path)\n",
    "        plt.subplot(len(npy_paths), 1, i + 1)\n",
    "        librosa.display.specshow(mel, x_axis='time', y_axis='mel', cmap='magma')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(titles[i] if titles else f\"Spectrogram {i+1}: {path}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17423bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_multiple_spectrograms(\n",
    "    [\n",
    "        \"D:\\Licenta\\Datasets\\Audio\\data\\MelSpectrograms/test\\Alzheimer/add_reverb_44_adrso144_shifted_part2.npy\",\n",
    "        \"D:\\Licenta\\Datasets\\Audio\\data\\MelSpectrograms/test\\Healthy/adrso002_vol_up_part4.npy\",\n",
    "        \"D:\\Licenta\\Datasets\\Audio\\data\\MelSpectrograms/test\\Parkinson/add_reverb_47_ID18_pd_4_3_3_readtext_pitch_down_part2.npy\"\n",
    "    ],\n",
    "    titles=[\"Alzheimer\", \"Parkinson\", \"Healthy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3539be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_random_predictions(model, dataloader, class_names, count=100):\n",
    "    model.eval()\n",
    "    all_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audio, mel, labels in dataloader:\n",
    "            audio, mel, labels = audio.to(device), mel.to(device), labels.to(device)\n",
    "            outputs = model(audio, mel)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                all_data.append({\n",
    "                    \"true\": labels[i].item(),\n",
    "                    \"pred\": preds[i].item(),\n",
    "                    \"conf\": probs[i][preds[i]].item()\n",
    "                })\n",
    "\n",
    "    sampled = random.sample(all_data, min(count, len(all_data)))\n",
    "\n",
    "    for i, d in enumerate(sampled):\n",
    "        correct = d[\"true\"] == d[\"pred\"]\n",
    "        label_color = \"\\033[92m\" if correct else \"\\033[91m\"  \n",
    "        print(f\"{label_color}[{i+1}] True: {class_names[d['true']]:10} | Pred: {class_names[d['pred']]:10} | Conf: {d['conf']:.2f}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_predictions(model, test_loader, class_names, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def show_wrong_spectrograms(model, dataloader, class_names, max_examples=100):\n",
    "    model.eval()\n",
    "    wrong_examples = []\n",
    "    total = 0\n",
    "    wrong = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audio, mel, labels in dataloader:\n",
    "            audio = audio.to(device)\n",
    "            mel = mel.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(audio, mel)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                total += 1\n",
    "                if preds[i] != labels[i]:\n",
    "                    wrong += 1\n",
    "                    wrong_examples.append({\n",
    "                        \"mel\": mel[i].cpu().numpy(),\n",
    "                        \"true\": labels[i].item(),\n",
    "                        \"pred\": preds[i].item()\n",
    "                    })\n",
    "                if len(wrong_examples) >= max_examples:\n",
    "                    break\n",
    "            if len(wrong_examples) >= max_examples:\n",
    "                break\n",
    "\n",
    "    print(f\"Predicții greșite: {wrong}/{total} ({(wrong/total)*100:.2f}%)\")\n",
    "\n",
    "    n = len(wrong_examples)\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "    plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "    for idx, item in enumerate(wrong_examples):\n",
    "        plt.subplot(rows, cols, idx + 1)\n",
    "        librosa.display.specshow(item[\"mel\"], x_axis='time', y_axis='mel', cmap='magma')\n",
    "        plt.title(f\"True: {class_names[item['true']]}\\nPred: {class_names[item['pred']]}\", color='red')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wrong_spectrograms(model, test_loader, class_names, max_examples=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
